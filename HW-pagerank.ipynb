{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORq76JBFYd8AL7SPgooeup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehdibenismail/M/blob/master/HW-pagerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPsdghnln9rf"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import numpy\n",
        "\n",
        "DAMPING = 0.85\n",
        "SAMPLES = 10000\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    if len(sys.argv) != 2:\n",
        "        sys.exit(\"Usage: python pagerank.py corpus\")\n",
        "    corpus = crawl(sys.argv[1])\n",
        "    \n",
        "    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)\n",
        "    print(f\"PageRank Results from Sampling (n = {SAMPLES})\")\n",
        "    for page in sorted(ranks):\n",
        "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
        "    ranks = iterate_pagerank(corpus, DAMPING)\n",
        "    print(f\"PageRank Results from Iteration\")\n",
        "    for page in sorted(ranks):\n",
        "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
        "\n",
        "\n",
        "def crawl(directory):\n",
        "    \"\"\"\n",
        "    Parse a directory of HTML pages and check for links to other pages.\n",
        "    Return a dictionary where each key is a page, and values are\n",
        "    a list of all other pages in the corpus that are linked to by the page.\n",
        "    \"\"\"\n",
        "    pages = dict()\n",
        "\n",
        "    \n",
        "    for filename in os.listdir(directory):\n",
        "        if not filename.endswith(\".html\"):\n",
        "            continue\n",
        "        with open(os.path.join(directory, filename)) as f:\n",
        "            contents = f.read()\n",
        "            links = re.findall(r\"<a\\s+(?:[^>]*?)href=\\\"([^\\\"]*)\\\"\", contents)\n",
        "            pages[filename] = set(links) - {filename}\n",
        "\n",
        "    # Only include links to other pages in the corpus\n",
        "    for filename in pages:\n",
        "        pages[filename] = set(\n",
        "            link for link in pages[filename]\n",
        "            if link in pages\n",
        "        )\n",
        "\n",
        "    return pages\n",
        "\n",
        "\n",
        "def transition_model(corpus, page, damping_factor):\n",
        "    \"\"\"\n",
        "    Return a probability distribution over which page to visit next,\n",
        "    given a current page.\n",
        "    With probability `damping_factor`, choose a link at random\n",
        "    linked to by `page`. With probability `1 - damping_factor`, choose\n",
        "    a link at random chosen from all pages in the corpus.\n",
        "    \"\"\"\n",
        "    \n",
        "    p = [damping_factor, 1 - damping_factor]\n",
        "    model = dict()\n",
        "    linked = set()\n",
        "    unlinked = set()\n",
        "    \n",
        "   \n",
        "    for key, value in corpus.items():\n",
        "\n",
        "        model[key] = 0.0\n",
        "\n",
        "        if key != page:\n",
        "            continue\n",
        "\n",
        "        linked = value\n",
        "        unlinked = set()\n",
        "        for key in corpus:\n",
        "            if key in linked:\n",
        "                continue\n",
        "            unlinked.add(key)\n",
        "\n",
        "   \n",
        "    linked_count = len(linked)\n",
        "    unlinked_count = len(unlinked)\n",
        "    total = linked_count + unlinked_count\n",
        "    base_prob = p[1] / total\n",
        "    for key in model:\n",
        "        \n",
        "        if key in linked:\n",
        "            model[key] = base_prob + damping_factor / linked_count\n",
        "        else:\n",
        "            model[key] = base_prob\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def sample_pagerank(corpus, damping_factor, n):\n",
        "    \"\"\"\n",
        "    Return PageRank values for each page by sampling `n` pages\n",
        "    according to transition model, starting with a page at random.\n",
        "    Return a dictionary where keys are page names, and values are\n",
        "    their estimated PageRank value (a value between 0 and 1). All\n",
        "    PageRank values should sum to 1.\n",
        "    \"\"\"\n",
        "    rank = dict()\n",
        "\n",
        "  \n",
        "    for key in corpus:\n",
        "        rank[key] = 0.0\n",
        "\n",
        "   \n",
        "    sample = random.choices(list(corpus))[0]\n",
        "    rank[sample] += (1 / n)\n",
        "\n",
        "   \n",
        "    for i in range(1, n):\n",
        "\n",
        "        model = transition_model(corpus, sample, damping_factor)\n",
        "        next_pages = []\n",
        "        probabilities = []\n",
        "        for key, value in model.items():\n",
        "            next_pages.append(key)\n",
        "            probabilities.append(value)\n",
        "        \n",
        "        sample = random.choices(next_pages, weights=probabilities)[0]\n",
        "        rank[sample] += (1 / n)\n",
        "\n",
        "    return rank"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}